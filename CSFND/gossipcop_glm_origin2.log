/home/myub2004/anaconda3/bin/conda run -n csfnd --no-capture-output python /mnt/c/Users/wdnmd/PycharmProjects/CSFND_yanjie/CSFND/run.py --dataset=gossipcop_glm_origin --batch-size=16 --inference=False --es-patience=9 --wd=1e-4
the ab hyperparameters UNSPR:True_MultiCLS:True_AGG:True_AVG:False
===> start training at:  0807-130739
===> process gossipcop_glm_origin data...
100%|██████████| 160/160 [00:03<00:00, 48.01it/s]
    load gossipcop_glm_origin 160   train data from files, include 80 fake and 80 real news.
===> process gossipcop_glm_origin data...
 24%|██▎       | 1033/4379 [00:28<01:54, 29.18it/s]/home/myub2004/anaconda3/envs/csfnd/lib/python3.7/site-packages/PIL/Image.py:997: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images
  "Palette images with Transparency expressed in bytes should be "
100%|██████████| 4379/4379 [01:59<00:00, 36.61it/s]
    load gossipcop_glm_origin 4379  test data from files, include 163 fake and 4216 real news.
===> process gossipcop_glm_origin data...
100%|██████████| 160/160 [00:02<00:00, 54.25it/s]
    load gossipcop_glm_origin 160   valid data from files, include 80 fake and 80 real news.
===> cluster text and image items and get the cluster pseudo-label.
Some weights of the model checkpoint at ./model/bert_base_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
===> perform the Unsupervised context learning to get the context features.
Some weights of the model checkpoint at ./model/bert_base_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
     epoch 0 | loss 4.889556, text 2.435914 image 2.453641 |.
     epoch 1 | loss 4.915202, text 2.370639 image 2.544563 |.
     epoch 2 | loss 4.869770, text 2.331132 image 2.538638 |.
     epoch 3 | loss 4.795561, text 2.349010 image 2.446551 |.
     epoch 4 | loss 4.883062, text 2.334789 image 2.548273 |.
     epoch 5 | loss 4.789406, text 2.454176 image 2.335230 |.
     epoch 6 | loss 4.923702, text 2.451053 image 2.472648 |.
     epoch 7 | loss 4.738838, text 2.208678 image 2.530160 |.
     epoch 8 | loss 4.494958, text 2.132262 image 2.362696 |.
     epoch 9 | loss 4.806967, text 2.252455 image 2.554512 |.
     epoch 10| loss 14.837432, text 6.746508 image 8.090923 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 11| loss 12.281616, text 5.042917 image 7.238699 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 12| loss 12.247897, text 4.465405 image 7.782492 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 13| loss 10.752601, text 4.656972 image 6.095629 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 14| loss 11.176983, text 5.035812 image 6.141171 |a larger loss, continue.
     epoch 15| loss 9.917554, text 3.995810 image 5.921744 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 16| loss 8.761528, text 4.697448 image 4.064080 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 17| loss 9.167199, text 3.726388 image 5.440811 |a larger loss, continue.
     epoch 18| loss 9.388562, text 4.073774 image 5.314788 |a larger loss, continue.
     epoch 19| loss 7.098491, text 3.249247 image 3.849244 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 20| loss 7.590303, text 3.448686 image 4.141618 |a larger loss, continue.
     epoch 21| loss 7.668310, text 3.094881 image 4.573429 |a larger loss, continue.
     epoch 22| loss 9.516528, text 3.912509 image 5.604019 |a larger loss, continue.
     epoch 23| loss 7.266186, text 3.410841 image 3.855345 |a larger loss, continue.
     epoch 24| loss 6.524216, text 2.884033 image 3.640183 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 25| loss 7.518270, text 2.887357 image 4.630912 |a larger loss, continue.
     epoch 26| loss 6.728646, text 2.969446 image 3.759200 |a larger loss, continue.
     epoch 27| loss 5.653934, text 2.910198 image 2.743736 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 28| loss 8.331018, text 4.143488 image 4.187530 |a larger loss, continue.
     epoch 29| loss 9.598850, text 4.848992 image 4.749858 |a larger loss, continue.
     epoch 30| loss 7.673670, text 3.478347 image 4.195323 |a larger loss, continue.
     epoch 31| loss 6.004665, text 2.437128 image 3.567537 |a larger loss, continue.
     epoch 32| loss 6.754801, text 3.593825 image 3.160975 |a larger loss, continue.
     epoch 33| loss 7.651758, text 2.612899 image 5.038858 |a larger loss, continue.
     epoch 34| loss 6.577967, text 3.460947 image 3.117020 |a larger loss, continue.
     epoch 35| loss 8.889651, text 3.913572 image 4.976079 |a larger loss, continue.
     epoch 36| loss 8.148534, text 3.281014 image 4.867520 |a larger loss, continue.
     epoch 37| loss 6.421778, text 2.527783 image 3.893995 |a larger loss, continue.
early stop with patience 10 at epoch  27
===> training the main model.
Some weights of the model checkpoint at ./model/bert_base_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    => select the primary modality: text
  Epoch 00| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 16.331654, con 3.446012, intra 6.386950, pred 6.498692, acc 0.503106, acc_real 0.370370, acc_fake 0.629630|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 16.820044, con 4.790047, intra 5.516256, pred 6.513742, acc 0.496894, acc_real 0.222222, acc_fake 0.765432
  Epoch 01| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 11.475308, con 2.739765, intra 4.541768, pred 4.193776, acc 0.664596, acc_real 0.555556, acc_fake 0.765432|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 16.193378, con 4.144266, intra 5.506302, pred 6.542810, acc 0.484472, acc_real 0.185185, acc_fake 0.777778
  Epoch 02| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 9.814914, con 1.455622, intra 4.149007, pred 4.210285, acc 0.633540, acc_real 0.506173, acc_fake 0.753086|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 16.058672, con 3.952627, intra 5.562704, pred 6.543339, acc 0.478261, acc_real 0.234568, acc_fake 0.716049
  Epoch 03| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 11.591594, con 2.812170, intra 4.546478, pred 4.232946, acc 0.627329, acc_real 0.530864, acc_fake 0.716049|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 16.096046, con 4.025033, intra 5.534293, pred 6.536719, acc 0.478261, acc_real 0.246914, acc_fake 0.703704
  Epoch 04| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 13.712229, con 3.234668, intra 5.611027, pred 4.866534, acc 0.708075, acc_real 0.580247, acc_fake 0.827160|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.943397, con 3.875453, intra 5.545056, pred 6.522888, acc 0.472050, acc_real 0.234568, acc_fake 0.703704
  Epoch 05| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 14.713555, con 3.276288, intra 5.812110, pred 5.625156, acc 0.677019, acc_real 0.580247, acc_fake 0.765432|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.769417, con 3.729706, intra 5.515234, pred 6.524477, acc 0.478261, acc_real 0.234568, acc_fake 0.716049
  Epoch 06| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 12.247159, con 3.193187, intra 4.915697, pred 4.138275, acc 0.732919, acc_real 0.666667, acc_fake 0.790123|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.902897, con 3.859385, intra 5.522126, pred 6.521387, acc 0.465839, acc_real 0.222222, acc_fake 0.703704
  Epoch 07| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 7.623184, con 1.327022, intra 3.565138, pred 2.731024, acc 0.732919, acc_real 0.641975, acc_fake 0.814815|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.823722, con 3.841969, intra 5.461814, pred 6.519938, acc 0.465839, acc_real 0.234568, acc_fake 0.691358
  Epoch 08| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 17.921399, con 3.937969, intra 7.786456, pred 6.196973, acc 0.695652, acc_real 0.617284, acc_fake 0.765432|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.538132, con 3.535861, intra 5.502342, pred 6.499928, acc 0.509317, acc_real 0.395062, acc_fake 0.617284
  Epoch 09| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 10.727059, con 2.254742, intra 4.414620, pred 4.057696, acc 0.807453, acc_real 0.765432, acc_fake 0.839506|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.381421, con 3.401274, intra 5.485727, pred 6.494420, acc 0.527950, acc_real 0.481481, acc_fake 0.567901
  Epoch 10| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 6.660331, con 1.102823, intra 2.836960, pred 2.720548, acc 0.745342, acc_real 0.753086, acc_fake 0.728395|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.362752, con 3.392835, intra 5.484135, pred 6.485782, acc 0.540373, acc_real 0.506173, acc_fake 0.567901
  Epoch 11| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 5.594433, con 1.311404, intra 2.295715, pred 1.987314, acc 0.788820, acc_real 0.802469, acc_fake 0.765432|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.716138, con 2.733175, intra 5.494106, pred 6.488857, acc 0.540373, acc_real 0.481481, acc_fake 0.592593
  Epoch 12| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 3.688388, con 0.563270, intra 1.769197, pred 1.355921, acc 0.813665, acc_real 0.790123, acc_fake 0.827160|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.101964, con 3.141387, intra 5.466020, pred 6.494557, acc 0.515528, acc_real 0.432099, acc_fake 0.592593
  Epoch 13| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 8.760147, con 1.626715, intra 3.814710, pred 3.318721, acc 0.782609, acc_real 0.753086, acc_fake 0.802469|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.045437, con 3.083972, intra 5.464678, pred 6.496788, acc 0.552795, acc_real 0.456790, acc_fake 0.641975
  Epoch 14| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 7.316874, con 1.370712, intra 3.252513, pred 2.693650, acc 0.807453, acc_real 0.777778, acc_fake 0.827160|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.237155, con 3.308019, intra 5.437428, pred 6.491708, acc 0.552795, acc_real 0.419753, acc_fake 0.679012
  Epoch 15| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 9.567610, con 2.171104, intra 4.012694, pred 3.383812, acc 0.807453, acc_real 0.790123, acc_fake 0.814815|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.131640, con 3.193345, intra 5.448793, pred 6.489501, acc 0.565217, acc_real 0.456790, acc_fake 0.666667
  Epoch 16| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 7.397765, con 1.192610, intra 3.508006, pred 2.697149, acc 0.844720, acc_real 0.839506, acc_fake 0.839506|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.106607, con 3.175945, intra 5.450694, pred 6.479967, acc 0.565217, acc_real 0.469136, acc_fake 0.654321
  Epoch 17| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 7.470469, con 1.003432, intra 3.791795, pred 2.675242, acc 0.832298, acc_real 0.839506, acc_fake 0.814815|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.742980, con 3.829289, intra 5.444471, pred 6.469220, acc 0.577640, acc_real 0.543210, acc_fake 0.604938
  Epoch 18| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 10.885259, con 2.138584, intra 4.767047, pred 3.979628, acc 0.875776, acc_real 0.876543, acc_fake 0.864198|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.738096, con 3.823920, intra 5.449142, pred 6.465036, acc 0.565217, acc_real 0.456790, acc_fake 0.666667
  Epoch 19| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 5.993837, con 1.259792, intra 2.695581, pred 2.038465, acc 0.826087, acc_real 0.802469, acc_fake 0.839506|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.776274, con 3.843388, intra 5.463960, pred 6.468925, acc 0.552795, acc_real 0.432099, acc_fake 0.666667
  Epoch 20| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 3.680413, con 0.805640, intra 1.544738, pred 1.330035, acc 0.857143, acc_real 0.814815, acc_fake 0.888889|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.452011, con 3.518827, intra 5.463380, pred 6.469805, acc 0.565217, acc_real 0.456790, acc_fake 0.666667
  Epoch 21| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 6.514646, con 0.898307, intra 2.962774, pred 2.653565, acc 0.869565, acc_real 0.839506, acc_fake 0.888889|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.499429, con 3.583946, intra 5.438804, pred 6.476679, acc 0.546584, acc_real 0.407407, acc_fake 0.679012
  Epoch 22| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 6.541010, con 1.016464, intra 2.860778, pred 2.663768, acc 0.869565, acc_real 0.851852, acc_fake 0.876543|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.404440, con 3.474236, intra 5.458533, pred 6.471669, acc 0.552795, acc_real 0.432099, acc_fake 0.666667
  Epoch 23| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 3.588050, con 0.456086, intra 1.829630, pred 1.302333, acc 0.838509, acc_real 0.839506, acc_fake 0.827160|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.613884, con 3.646824, intra 5.493756, pred 6.473303, acc 0.565217, acc_real 0.456790, acc_fake 0.666667
  Epoch 24| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 5.597766, con 1.054405, intra 2.581114, pred 1.962247, acc 0.875776, acc_real 0.864198, acc_fake 0.876543|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.739277, con 3.743975, intra 5.521425, pred 6.473878, acc 0.565217, acc_real 0.469136, acc_fake 0.654321
  Epoch 25| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 1.614261, con 0.230122, intra 0.717354, pred 0.666785, acc 0.888199, acc_real 0.839506, acc_fake 0.925926|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.949697, con 3.973544, intra 5.507594, pred 6.468559, acc 0.546584, acc_real 0.444444, acc_fake 0.641975
  Epoch 26| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 5.587719, con 1.495553, intra 2.114028, pred 1.978139, acc 0.888199, acc_real 0.876543, acc_fake 0.888889|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.793958, con 3.821358, intra 5.506119, pred 6.466480, acc 0.559006, acc_real 0.469136, acc_fake 0.641975
  training stop in epoch 17.
  the best model had been saved to  ./FinalModel_gossipcop_glm_origin_clu17_lClu0.2_lTri0.6_UNSPRTrue_MultiCLSTrue_AGGTrue_AVGFalse.pt
===> construct multiple binary classifiers to detect fake news within each news cluster.
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
--- clu  0|4    fake, 1    real in 5    train| 8    fake, 43   real in 51   test
          pos:tn 6   fp 2   fn 38  tp 5  |acc 0.215686 acc_real 0.714286 acc_fake 0.136364
--- clu  1|1    fake, 2    real in 3    train| 3    fake, 60   real in 63   test
          pos:tn 1   fp 2   fn 13  tp 47 |acc 0.761905 acc_real 0.959184 acc_fake 0.071429
--- clu  2|0    fake, 7    real in 7    train| 3    fake, 128  real in 131  test
          pos:tn 0   fp 3   fn 0   tp 128|acc 0.977099 acc_real 0.977099 acc_fake 0.000000
--- clu  3|0    fake, 3    real in 3    train| 3    fake, 29   real in 32   test
          pos:tn 0   fp 3   fn 0   tp 29 |acc 0.906250 acc_real 0.906250 acc_fake 0.000000
--- clu  4|12   fake, 3    real in 15   train| 10   fake, 190  real in 200  test
          pos:tn 10  fp 0   fn 188 tp 2  |acc 0.060000 acc_real 1.000000 acc_fake 0.050505
--- clu  5|6    fake, 4    real in 10   train| 9    fake, 215  real in 224  test
          pos:tn 7   fp 2   fn 158 tp 57 |acc 0.285714 acc_real 0.966102 acc_fake 0.042424
--- clu  6|2    fake, 2    real in 4    train| 7    fake, 188  real in 195  test
          pos:tn 5   fp 2   fn 65  tp 123|acc 0.656410 acc_real 0.984000 acc_fake 0.071429
--- clu  7|5    fake, 7    real in 12   train| 9    fake, 399  real in 408  test
          pos:tn 5   fp 4   fn 181 tp 218|acc 0.546569 acc_real 0.981982 acc_fake 0.026882
--- clu  8|8    fake, 8    real in 16   train| 9    fake, 362  real in 371  test
          pos:tn 4   fp 5   fn 121 tp 241|acc 0.660377 acc_real 0.979675 acc_fake 0.032000
--- clu  9|0    fake, 1    real in 1    train| 0    fake, 1    real in 1    test
          pos:tn 0   fp 0   fn 0   tp 1  |acc 1.000000 acc_real 1.000000 acc_fake 0.000000
--- clu 10|4    fake, 4    real in 8    train| 5    fake, 295  real in 300  test
          pos:tn 4   fp 1   fn 179 tp 116|acc 0.400000 acc_real 0.991453 acc_fake 0.021858
--- clu 11|9    fake, 13   real in 22   train| 37   fake, 1265 real in 1302 test
          pos:tn 0   fp 37  fn 86  tp 1179|acc 0.905530 acc_real 0.969572 acc_fake 0.000000
--- clu 12|2    fake, 0    real in 2    train| 3    fake, 0    real in 3    test
          pos:tn 3   fp 0   fn 0   tp 0  |acc 1.000000 acc_real 0.000000 acc_fake 1.000000
--- clu 13|6    fake, 12   real in 18   train| 25   fake, 587  real in 612  test
          pos:tn 4   fp 21  fn 59  tp 528|acc 0.869281 acc_real 0.961749 acc_fake 0.063492
--- clu 14|This cluster has no test data.
--- clu 15|9    fake, 9    real in 18   train| 14   fake, 295  real in 309  test
          pos:tn 4   fp 10  fn 78  tp 217|acc 0.715210 acc_real 0.955947 acc_fake 0.048780
--- clu 16|11   fake, 4    real in 15   train| 18   fake, 159  real in 177  test
          pos:tn 14  fp 4   fn 107 tp 52 |acc 0.372881 acc_real 0.928571 acc_fake 0.115702
--- overall acc 0.687372 acc_real 0.968411 acc_fake 0.050000
===> final test results: acc 0.6874 acc_real 0.9684 acc_fake 0.0500 .

进程已结束，退出代码为 0
