/home/myub2004/anaconda3/bin/conda run -n csfnd --no-capture-output python /mnt/c/Users/wdnmd/PycharmProjects/CSFND_yanjie/CSFND/run.py --dataset=gossipcop_glm_origin --batch-size=16 --inference=False --es-patience=9 --wd=1e-4 
the ab hyperparameters UNSPR:True_MultiCLS:True_AGG:True_AVG:False
===> start training at:  0807-132903
===> process gossipcop_glm_origin data...
100%|██████████| 160/160 [00:04<00:00, 37.17it/s]
    load gossipcop_glm_origin 160   train data from files, include 80 fake and 80 real news.
===> process gossipcop_glm_origin data...
100%|██████████| 326/326 [00:09<00:00, 34.07it/s]
    load gossipcop_glm_origin 326   test data from files, include 163 fake and 163 real news.
===> process gossipcop_glm_origin data...
100%|██████████| 160/160 [00:03<00:00, 49.21it/s]
    load gossipcop_glm_origin 160   valid data from files, include 80 fake and 80 real news.
===> cluster text and image items and get the cluster pseudo-label.
Some weights of the model checkpoint at ./model/bert_base_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
===> perform the Unsupervised context learning to get the context features.
Some weights of the model checkpoint at ./model/bert_base_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
     epoch 0 | loss 4.753504, text 2.287053 image 2.466451 |.
     epoch 1 | loss 4.889523, text 2.419855 image 2.469669 |.
     epoch 2 | loss 4.764638, text 2.348338 image 2.416300 |.
     epoch 3 | loss 4.789574, text 2.387489 image 2.402084 |.
     epoch 4 | loss 4.645534, text 2.131117 image 2.514418 |.
     epoch 5 | loss 4.613028, text 2.358230 image 2.254798 |.
     epoch 6 | loss 4.879150, text 2.478867 image 2.400284 |.
     epoch 7 | loss 4.796002, text 2.460022 image 2.335980 |.
     epoch 8 | loss 4.487901, text 2.267638 image 2.220263 |.
     epoch 9 | loss 4.617490, text 2.365690 image 2.251801 |.
     epoch 10| loss 14.410934, text 5.994682 image 8.416252 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 11| loss 12.398430, text 4.442752 image 7.955678 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 12| loss 10.954042, text 4.012272 image 6.941770 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 13| loss 11.398112, text 3.940649 image 7.457463 |a larger loss, continue.
     epoch 14| loss 11.483658, text 4.198989 image 7.284670 |a larger loss, continue.
     epoch 15| loss 12.156373, text 4.702706 image 7.453668 |a larger loss, continue.
     epoch 16| loss 10.028277, text 4.017418 image 6.010858 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 17| loss 9.694820, text 3.059755 image 6.635065 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 18| loss 5.869471, text 2.388174 image 3.481297 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 19| loss 10.427374, text 4.191010 image 6.236364 |a larger loss, continue.
     epoch 20| loss 9.029720, text 3.215431 image 5.814288 |a larger loss, continue.
     epoch 21| loss 11.455427, text 4.772561 image 6.682867 |a larger loss, continue.
     epoch 22| loss 5.997081, text 2.635882 image 3.361198 |a larger loss, continue.
     epoch 23| loss 6.897192, text 2.825093 image 4.072098 |a larger loss, continue.
     epoch 24| loss 4.711337, text 2.210822 image 2.500514 |a lower loss, save model to: ./gossipcop_glm_origin_bs16_Clu17_unsupervised.pt
     epoch 25| loss 8.770905, text 2.649609 image 6.121296 |a larger loss, continue.
     epoch 26| loss 5.752448, text 1.803474 image 3.948974 |a larger loss, continue.
     epoch 27| loss 7.575317, text 2.968286 image 4.607031 |a larger loss, continue.
     epoch 28| loss 8.581310, text 4.110732 image 4.470578 |a larger loss, continue.
     epoch 29| loss 7.837922, text 2.412522 image 5.425400 |a larger loss, continue.
     epoch 30| loss 6.358243, text 2.830819 image 3.527423 |a larger loss, continue.
     epoch 31| loss 6.943187, text 2.136760 image 4.806427 |a larger loss, continue.
     epoch 32| loss 6.454752, text 2.820072 image 3.634681 |a larger loss, continue.
     epoch 33| loss 5.923339, text 1.850157 image 4.073182 |a larger loss, continue.
     epoch 34| loss 8.844524, text 4.255978 image 4.588547 |a larger loss, continue.
early stop with patience 10 at epoch  24
===> training the main model.
Some weights of the model checkpoint at ./model/bert_base_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    => select the primary modality: text
  Epoch 00| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 14.479919, con 3.460344, intra 5.266541, pred 5.753033, acc 0.546584, acc_real 0.407407, acc_fake 0.679012|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.693571, con 3.990672, intra 4.909516, pred 5.793383, acc 0.509317, acc_real 0.333333, acc_fake 0.679012
  Epoch 01| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 13.728438, con 3.593927, intra 5.202149, pred 4.932363, acc 0.639752, acc_real 0.592593, acc_fake 0.679012|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.128836, con 3.614450, intra 4.762982, pred 5.751404, acc 0.546584, acc_real 0.419753, acc_fake 0.666667
  Epoch 02| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 14.404770, con 3.305534, intra 5.463287, pred 5.635950, acc 0.639752, acc_real 0.580247, acc_fake 0.691358|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.222671, con 3.716835, intra 4.756666, pred 5.749168, acc 0.546584, acc_real 0.444444, acc_fake 0.641975
  Epoch 03| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 11.637242, con 2.713686, intra 4.753181, pred 4.170375, acc 0.701863, acc_real 0.641975, acc_fake 0.753086|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.038691, con 3.534538, intra 4.769138, pred 5.735015, acc 0.559006, acc_real 0.481481, acc_fake 0.629630
  Epoch 04| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 15.217141, con 3.385883, intra 6.195828, pred 5.635430, acc 0.645963, acc_real 0.604938, acc_fake 0.679012|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.241137, con 3.757147, intra 4.756641, pred 5.727348, acc 0.583851, acc_real 0.506173, acc_fake 0.654321
  Epoch 05| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 12.703465, con 2.602105, intra 5.222913, pred 4.878447, acc 0.701863, acc_real 0.679012, acc_fake 0.716049|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.735894, con 4.213374, intra 4.798003, pred 5.724518, acc 0.571429, acc_real 0.456790, acc_fake 0.679012
  Epoch 06| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 9.972597, con 2.200395, intra 4.354304, pred 3.417898, acc 0.695652, acc_real 0.691358, acc_fake 0.691358|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.648717, con 4.150900, intra 4.776487, pred 5.721331, acc 0.571429, acc_real 0.469136, acc_fake 0.666667
  Epoch 07| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 10.899035, con 2.282542, intra 4.422319, pred 4.194173, acc 0.745342, acc_real 0.753086, acc_fake 0.728395|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.589575, con 4.076289, intra 4.794085, pred 5.719202, acc 0.583851, acc_real 0.481481, acc_fake 0.679012
  Epoch 08| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 10.150203, con 2.730334, intra 3.995378, pred 3.424491, acc 0.770186, acc_real 0.765432, acc_fake 0.765432|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.525764, con 3.983758, intra 4.818825, pred 5.723181, acc 0.577640, acc_real 0.444444, acc_fake 0.703704
  Epoch 09| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 9.655598, con 2.245856, intra 4.003332, pred 3.406410, acc 0.739130, acc_real 0.740741, acc_fake 0.728395|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.782156, con 4.226446, intra 4.837777, pred 5.717933, acc 0.565217, acc_real 0.419753, acc_fake 0.703704
  Epoch 10| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 7.358096, con 1.364994, intra 3.291763, pred 2.701340, acc 0.763975, acc_real 0.740741, acc_fake 0.777778|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.760116, con 4.203708, intra 4.831563, pred 5.724844, acc 0.552795, acc_real 0.395062, acc_fake 0.703704
  Epoch 11| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 5.222473, con 1.055393, intra 2.062750, pred 2.104329, acc 0.745342, acc_real 0.728395, acc_fake 0.753086|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.689596, con 4.150095, intra 4.816634, pred 5.722866, acc 0.565217, acc_real 0.407407, acc_fake 0.716049
  Epoch 12| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 6.939963, con 0.852855, intra 3.303010, pred 2.784098, acc 0.757764, acc_real 0.777778, acc_fake 0.728395|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.625892, con 4.106174, intra 4.795301, pred 5.724416, acc 0.546584, acc_real 0.345679, acc_fake 0.740741
  Epoch 13| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 5.544377, con 0.805715, intra 2.691187, pred 2.047474, acc 0.782609, acc_real 0.777778, acc_fake 0.777778|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.704422, con 4.128648, intra 4.853988, pred 5.721786, acc 0.559006, acc_real 0.395062, acc_fake 0.716049
  Epoch 14| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 5.279933, con 0.781417, intra 2.479511, pred 2.019004, acc 0.838509, acc_real 0.839506, acc_fake 0.827160|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.512390, con 3.942899, intra 4.848507, pred 5.720985, acc 0.577640, acc_real 0.432099, acc_fake 0.716049
  Epoch 15| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 5.403954, con 1.012341, intra 2.385908, pred 2.005705, acc 0.795031, acc_real 0.827160, acc_fake 0.753086|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 14.701577, con 4.156715, intra 4.829343, pred 5.715519, acc 0.571429, acc_real 0.419753, acc_fake 0.716049
  Epoch 16| use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
train loss 5.285979, con 0.901032, intra 2.360988, pred 2.023959, acc 0.795031, acc_real 0.802469, acc_fake 0.777778|use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
valid loss 15.094023, con 4.521626, intra 4.857978, pred 5.714420, acc 0.571429, acc_real 0.432099, acc_fake 0.703704
  training stop in epoch 7.
  the best model had been saved to  ./FinalModel_gossipcop_glm_origin_clu17_lClu0.2_lTri0.6_UNSPRTrue_MultiCLSTrue_AGGTrue_AVGFalse.pt
===> construct multiple binary classifiers to detect fake news within each news cluster.
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
use the attention agg context information
--- clu  0|7    fake, 1    real in 8    train| 13   fake, 8    real in 21   test
          pos:tn 10  fp 3   fn 3   tp 5  |acc 0.714286 acc_real 0.625000 acc_fake 0.769231
--- clu  1|0    fake, 4    real in 4    train| 0    fake, 2    real in 2    test
          pos:tn 0   fp 0   fn 0   tp 2  |acc 1.000000 acc_real 1.000000 acc_fake 0.000000
--- clu  2|8    fake, 20   real in 28   train| 35   fake, 53   real in 88   test
          pos:tn 0   fp 35  fn 1   tp 52 |acc 0.590909 acc_real 0.597701 acc_fake 0.000000
--- clu  3|12   fake, 1    real in 13   train| 9    fake, 5    real in 14   test
          pos:tn 7   fp 2   fn 2   tp 3  |acc 0.714286 acc_real 0.600000 acc_fake 0.777778
--- clu  4|1    fake, 3    real in 4    train| 4    fake, 7    real in 11   test
          pos:tn 4   fp 0   fn 2   tp 5  |acc 0.818182 acc_real 1.000000 acc_fake 0.666667
--- clu  5|6    fake, 5    real in 11   train| 10   fake, 10   real in 20   test
          pos:tn 7   fp 3   fn 1   tp 9  |acc 0.800000 acc_real 0.750000 acc_fake 0.875000
--- clu  6|5    fake, 6    real in 11   train| 6    fake, 7    real in 13   test
          pos:tn 6   fp 0   fn 6   tp 1  |acc 0.538462 acc_real 1.000000 acc_fake 0.500000
--- clu  7|8    fake, 6    real in 14   train| 9    fake, 9    real in 18   test
          pos:tn 6   fp 3   fn 6   tp 3  |acc 0.500000 acc_real 0.500000 acc_fake 0.500000
--- clu  8|3    fake, 6    real in 9    train| 10   fake, 11   real in 21   test
          pos:tn 2   fp 8   fn 0   tp 11 |acc 0.619048 acc_real 0.578947 acc_fake 1.000000
--- clu  9|0    fake, 1    real in 1    train| 0    fake, 2    real in 2    test
          pos:tn 0   fp 0   fn 0   tp 2  |acc 1.000000 acc_real 1.000000 acc_fake 0.000000
--- clu 10|1    fake, 4    real in 5    train| 1    fake, 5    real in 6    test
          pos:tn 1   fp 0   fn 0   tp 5  |acc 1.000000 acc_real 1.000000 acc_fake 1.000000
--- clu 11|9    fake, 7    real in 16   train| 27   fake, 13   real in 40   test
          pos:tn 18  fp 9   fn 10  tp 3  |acc 0.525000 acc_real 0.250000 acc_fake 0.642857
--- clu 12|This cluster has no test data.
--- clu 13|3    fake, 0    real in 3    train| 3    fake, 0    real in 3    test
          pos:tn 3   fp 0   fn 0   tp 0  |acc 1.000000 acc_real 0.000000 acc_fake 1.000000
--- clu 14|3    fake, 0    real in 3    train| 4    fake, 0    real in 4    test
          pos:tn 4   fp 0   fn 0   tp 0  |acc 1.000000 acc_real 0.000000 acc_fake 1.000000
--- clu 15|4    fake, 0    real in 4    train| 2    fake, 2    real in 4    test
          pos:tn 2   fp 0   fn 2   tp 0  |acc 0.500000 acc_real 0.000000 acc_fake 0.500000
--- clu 16|10   fake, 15   real in 25   train| 30   fake, 29   real in 59   test
          pos:tn 8   fp 22  fn 4   tp 25 |acc 0.559322 acc_real 0.531915 acc_fake 0.666667
--- overall acc 0.625767 acc_real 0.597156 acc_fake 0.678261
===> final test results: acc 0.6258 acc_real 0.5972 acc_fake 0.6783 .

进程已结束，退出代码为 0
